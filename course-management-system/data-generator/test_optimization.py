# file: data-generator/test_optimization.py
# åŠŸèƒ½: æµ‹è¯•ä¼˜åŒ–åçš„æ•°æ®ç”Ÿæˆè„šæœ¬

import sys
import time
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from optimized_main import OptimizedDataGenerator
from main import generate_complete_dataset
from generators.quality_assessment import DataQualityAssessment


def compare_data_quality():
    """å¯¹æ¯”åŸå§‹å’Œä¼˜åŒ–åçš„æ•°æ®è´¨é‡"""
    print("ğŸ”¬ å¼€å§‹æ•°æ®è´¨é‡å¯¹æ¯”æµ‹è¯•...")
    print("="*60)
    
    quality_assessor = DataQualityAssessment()
    
    # 1. æµ‹è¯•åŸå§‹æ•°æ®ç”Ÿæˆå™¨
    print("\nğŸ“Š æµ‹è¯•åŸå§‹æ•°æ®ç”Ÿæˆå™¨...")
    start_time = time.time()
    try:
        original_dataset = generate_complete_dataset(
            scale='small',
            output_formats=[],
            validate_data=False
        )
        original_time = time.time() - start_time
        original_quality = quality_assessor.evaluate_data_quality(original_dataset)
        print(f"âœ… åŸå§‹ç”Ÿæˆå™¨å®Œæˆï¼Œè€—æ—¶: {original_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ åŸå§‹ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
        return
    
    # 2. æµ‹è¯•ä¼˜åŒ–æ•°æ®ç”Ÿæˆå™¨
    print("\nğŸš€ æµ‹è¯•ä¼˜åŒ–æ•°æ®ç”Ÿæˆå™¨...")
    start_time = time.time()
    try:
        optimized_generator = OptimizedDataGenerator()
        optimized_dataset = optimized_generator.generate_enhanced_dataset(
            scale='small',
            conflict_difficulty='mixed'
        )
        optimized_time = time.time() - start_time
        optimized_quality = optimized_dataset.get('quality_report', {}).get('detailed_scores', {})
        print(f"âœ… ä¼˜åŒ–ç”Ÿæˆå™¨å®Œæˆï¼Œè€—æ—¶: {optimized_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
        return
    
    # 3. å¯¹æ¯”ç»“æœ
    print("\nğŸ“‹ è´¨é‡å¯¹æ¯”ç»“æœ:")
    print("-"*60)
    print(f"{'æŒ‡æ ‡':<15} {'åŸå§‹':<10} {'ä¼˜åŒ–å':<10} {'æ”¹è¿›':<10}")
    print("-"*60)
    
    metrics = ['realism', 'complexity', 'diversity', 'consistency', 'algorithm_stress']
    
    for metric in metrics:
        original_score = getattr(original_quality, f'{metric}_score', 0)
        optimized_score = optimized_quality.get(metric, 0)
        improvement = optimized_score - original_score
        
        print(f"{metric:<15} {original_score:<10.3f} {optimized_score:<10.3f} {improvement:+.3f}")
    
    overall_original = original_quality.overall_score
    overall_optimized = optimized_quality.get('overall_score', 0)
    overall_improvement = overall_optimized - overall_original
    
    print("-"*60)
    print(f"{'æ€»ä½“åˆ†æ•°':<15} {overall_original:<10.3f} {overall_optimized:<10.3f} {overall_improvement:+.3f}")
    
    # 4. æ€§èƒ½å¯¹æ¯”
    print(f"\nâ±ï¸  æ€§èƒ½å¯¹æ¯”:")
    print(f"   åŸå§‹ç”Ÿæˆå™¨: {original_time:.2f}ç§’")
    print(f"   ä¼˜åŒ–ç”Ÿæˆå™¨: {optimized_time:.2f}ç§’")
    print(f"   æ—¶é—´å˜åŒ–: {optimized_time - original_time:+.2f}ç§’")
    
    # 5. æ•°æ®è§„æ¨¡å¯¹æ¯”
    print(f"\nğŸ“Š æ•°æ®è§„æ¨¡å¯¹æ¯”:")
    original_counts = {
        'students': len(original_dataset.get('students', [])),
        'teachers': len(original_dataset.get('teachers', [])),
        'courses': len(original_dataset.get('courses', [])),
        'enrollments': len(original_dataset.get('enrollments', []))
    }
    
    optimized_counts = {
        'students': len(optimized_dataset.get('students', [])),
        'teachers': len(optimized_dataset.get('teachers', [])),
        'courses': len(optimized_dataset.get('courses', [])),
        'enrollments': len(optimized_dataset.get('enrollments', []))
    }
    
    for data_type in original_counts:
        orig_count = original_counts[data_type]
        opt_count = optimized_counts[data_type]
        print(f"   {data_type}: {orig_count} -> {opt_count}")
    
    # 6. æ–°å¢åŠŸèƒ½å±•ç¤º
    print(f"\nğŸ†• ä¼˜åŒ–ç‰ˆæœ¬æ–°å¢åŠŸèƒ½:")
    new_features = [
        f"æ•™å¸ˆæ—¶é—´åå¥½: {len(optimized_dataset.get('teacher_preferences', []))} æ¡",
        f"è¯¾ç¨‹ä¾èµ–å…³ç³»: {len(optimized_dataset.get('course_dependencies', {}))} ä¸ªè¯¾ç¨‹",
        f"æ•™å¸ˆèƒ½åŠ›æ¡£æ¡ˆ: {len(optimized_dataset.get('teacher_competencies', {}))} ä¸ªæ•™å¸ˆ",
        f"å†²çªåœºæ™¯: {len(optimized_dataset.get('conflicts', []))} ä¸ª",
        f"è´¨é‡è¯„ä¼°æŠ¥å‘Š: {'æœ‰' if 'quality_report' in optimized_dataset else 'æ— '}"
    ]
    
    for feature in new_features:
        print(f"   âœ¨ {feature}")
    
    print(f"\nğŸ¯ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
    if overall_improvement > 0.1:
        print("   ğŸ† ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼è´¨é‡æå‡è¶…è¿‡10%")
    elif overall_improvement > 0.05:
        print("   ğŸ‘ ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼è´¨é‡æå‡5-10%")
    elif overall_improvement > 0:
        print("   â˜‘ï¸  ä¼˜åŒ–æ•ˆæœä¸€èˆ¬ï¼Œè´¨é‡ç•¥æœ‰æå‡")
    else:
        print("   âš ï¸  ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")


if __name__ == "__main__":
    compare_data_quality()