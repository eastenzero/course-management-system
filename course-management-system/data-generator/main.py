# file: data-generator/main.py
# åŠŸèƒ½: ä¸»æ•°æ®ç”Ÿæˆè„šæœ¬

import argparse
import sys
import time
from typing import Dict, Any, List
from datetime import datetime
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from config import DATA_SCALE_CONFIG, GenerationConfig
from generators import (
    DepartmentGenerator,
    UserGenerator,
    CourseGenerator,
    FacilityGenerator,
    ComplexScenarioGenerator,
    DataExporter
)


def generate_complete_dataset(scale: str = 'large', 
                            output_formats: List[str] = None,
                            output_dir: str = 'output',
                            validate_data: bool = True,
                            include_conflicts: bool = True) -> Dict[str, Any]:
    """ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•æ•°æ®é›†
    
    Args:
        scale: æ•°æ®è§„æ¨¡ ('large', 'medium', 'small')
        output_formats: è¾“å‡ºæ ¼å¼åˆ—è¡¨ (['json', 'sql'])
        output_dir: è¾“å‡ºç›®å½•
        validate_data: æ˜¯å¦éªŒè¯æ•°æ®
        include_conflicts: æ˜¯å¦åŒ…å«å†²çªåœºæ™¯
        
    Returns:
        ç”Ÿæˆçš„æ•°æ®é›†å­—å…¸
    """
    if output_formats is None:
        output_formats = ['json']
    
    # éªŒè¯è§„æ¨¡å‚æ•°
    if scale not in DATA_SCALE_CONFIG:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®è§„æ¨¡: {scale}ã€‚æ”¯æŒçš„è§„æ¨¡: {list(DATA_SCALE_CONFIG.keys())}")
    
    config = DATA_SCALE_CONFIG[scale]
    
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ {scale} è§„æ¨¡æµ‹è¯•æ•°æ®...")
    print(f"ğŸ“Š æ•°æ®è§„æ¨¡é…ç½®: {config}")
    print("-" * 60)
    
    start_time = time.time()
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    print("ğŸ”§ åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨...")
    dept_gen = DepartmentGenerator()
    user_gen = UserGenerator()
    course_gen = CourseGenerator()
    facility_gen = FacilityGenerator()
    scenario_gen = ComplexScenarioGenerator()
    exporter = DataExporter(output_dir)
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    print("\nğŸ“š ç”Ÿæˆé™¢ç³»ä¸“ä¸šæ•°æ®...")
    departments = dept_gen.generate_departments(config['departments'])
    majors = dept_gen.generate_majors(departments)
    print(f"   âœ… ç”Ÿæˆ {len(departments)} ä¸ªé™¢ç³»ï¼Œ{len(majors)} ä¸ªä¸“ä¸š")
    
    print("\nğŸ‘¥ ç”Ÿæˆç”¨æˆ·æ•°æ®...")
    students = user_gen.generate_students(config['students'], majors)
    teachers = user_gen.generate_teachers(config['teachers'], departments)
    print(f"   âœ… ç”Ÿæˆ {len(students)} åå­¦ç”Ÿï¼Œ{len(teachers)} åæ•™å¸ˆ")
    
    print("\nğŸ“– ç”Ÿæˆè¯¾ç¨‹æ•°æ®...")
    courses = course_gen.generate_courses(config['courses'], departments, teachers)
    print(f"   âœ… ç”Ÿæˆ {len(courses)} é—¨è¯¾ç¨‹")
    
    print("\nğŸ¢ ç”Ÿæˆè®¾æ–½æ•°æ®...")
    classrooms = facility_gen.generate_classrooms(config['classrooms'])
    time_slots = facility_gen.generate_time_slots()
    print(f"   âœ… ç”Ÿæˆ {len(classrooms)} é—´æ•™å®¤ï¼Œ{len(time_slots)} ä¸ªæ—¶é—´æ®µ")
    
    print("\nğŸ¯ ç”Ÿæˆå¤æ‚åœºæ™¯æ•°æ®...")
    enrollments = scenario_gen.generate_enrollment_data(students, courses)
    teacher_preferences = scenario_gen.generate_teacher_preferences(teachers, time_slots)
    print(f"   âœ… ç”Ÿæˆ {len(enrollments)} æ¡é€‰è¯¾è®°å½•ï¼Œ{len(teacher_preferences)} æ¡æ•™å¸ˆåå¥½")
    
    conflicts = []
    constraints = []
    if include_conflicts:
        conflicts = scenario_gen.generate_conflict_scenarios(courses, teachers, classrooms, students)
        constraints = scenario_gen.generate_scheduling_constraints(courses, teachers, classrooms)
        print(f"   âœ… ç”Ÿæˆ {len(conflicts)} ä¸ªå†²çªåœºæ™¯ï¼Œ{len(constraints)} ä¸ªçº¦æŸæ¡ä»¶")
    
    # ç»„è£…å®Œæ•´æ•°æ®é›†
    dataset = {
        'departments': departments,
        'majors': majors,
        'students': students,
        'teachers': teachers,
        'courses': courses,
        'classrooms': classrooms,
        'time_slots': time_slots,
        'enrollments': enrollments,
        'teacher_preferences': teacher_preferences,
        'conflicts': conflicts,
        'constraints': constraints,
    }

    # è®¡ç®—æ€»è®°å½•æ•°
    total_records = sum(len(v) if isinstance(v, list) else 0 for v in dataset.values() if v)

    # æ·»åŠ å…ƒæ•°æ®
    dataset['metadata'] = {
        'scale': scale,
        'generated_at': datetime.now().isoformat(),
        'generator_version': '1.0.0',
        'config': config,
        'total_records': total_records,
        'generation_time_seconds': 0,  # å°†åœ¨åé¢æ›´æ–°
        'validation_passed': False,  # å°†åœ¨éªŒè¯åæ›´æ–°
        'output_formats': output_formats,
        'include_conflicts': include_conflicts
    }
    
    generation_time = time.time() - start_time
    dataset['metadata']['generation_time_seconds'] = round(generation_time, 2)
    
    total_records = dataset['metadata']['total_records']
    print(f"\nâœ¨ æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f"   ğŸ“Š æ€»è®¡ {total_records:,} æ¡è®°å½•")
    print(f"   â±ï¸  è€—æ—¶ {generation_time:.2f} ç§’")
    print(f"   ğŸš€ ç”Ÿæˆé€Ÿåº¦ {total_records/generation_time:.0f} æ¡/ç§’")
    
    # æ•°æ®éªŒè¯
    validation_errors = {}
    if validate_data:
        print("\nğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        validation_start = time.time()
        validation_errors = exporter.validate_data_integrity(dataset)
        validation_time = time.time() - validation_start
        
        if validation_errors and any(validation_errors.values()):
            print(f"   âš ï¸  å‘ç° {sum(len(errors) for errors in validation_errors.values())} ä¸ªé—®é¢˜")
            for table, errors in validation_errors.items():
                if errors:
                    print(f"      - {table}: {len(errors)} ä¸ªé—®é¢˜")
            dataset['metadata']['validation_passed'] = False
        else:
            print(f"   âœ… æ•°æ®éªŒè¯é€šè¿‡ (è€—æ—¶ {validation_time:.2f} ç§’)")
            dataset['metadata']['validation_passed'] = True
    
    # å¯¼å‡ºæ•°æ®
    if output_formats:
        print(f"\nğŸ’¾ å¯¼å‡ºæ•°æ® (æ ¼å¼: {', '.join(output_formats)})...")
        export_start = time.time()
        
        exported_files = []
        if 'json' in output_formats:
            json_file = exporter.export_to_json(dataset)
            exported_files.append(json_file)
        
        if 'sql' in output_formats:
            sql_file = exporter.export_to_sql(dataset)
            exported_files.append(sql_file)
        
        # ç”Ÿæˆæ•°æ®æŠ¥å‘Š
        report_file = exporter.generate_data_report(dataset, validation_errors)
        exported_files.append(report_file)
        
        export_time = time.time() - export_start
        print(f"   âœ… å¯¼å‡ºå®Œæˆ (è€—æ—¶ {export_time:.2f} ç§’)")
        print(f"   ğŸ“ æ–‡ä»¶åˆ—è¡¨:")
        for file_path in exported_files:
            print(f"      - {file_path}")
    
    total_time = time.time() - start_time
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶ {total_time:.2f} ç§’")
    print("-" * 60)
    
    return dataset


def main(args=None):
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(
        description='æ ¡å›­è¯¾ç¨‹è¡¨ç®¡ç†ç³»ç»Ÿæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py --scale large --format json sql
  python main.py --scale medium --output-dir ./data --no-validate
  python main.py --scale small --no-conflicts
        """
    )
    
    parser.add_argument(
        '--scale', '-s',
        choices=['large', 'medium', 'small'],
        default='medium',
        help='æ•°æ®è§„æ¨¡ (é»˜è®¤: medium)'
    )
    
    parser.add_argument(
        '--format', '-f',
        nargs='+',
        choices=['json', 'sql'],
        default=['json'],
        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: json)'
    )
    
    parser.add_argument(
        '--output-dir', '-o',
        default='output',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: output)'
    )
    
    parser.add_argument(
        '--no-validate',
        action='store_true',
        help='è·³è¿‡æ•°æ®éªŒè¯'
    )
    
    parser.add_argument(
        '--no-conflicts',
        action='store_true',
        help='ä¸ç”Ÿæˆå†²çªåœºæ™¯æ•°æ®'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º'
    )
    
    parser.add_argument(
        '--version', '-v',
        action='version',
        version='æ•°æ®ç”Ÿæˆå™¨ v1.0.0'
    )
    
    if args is None:
        args = sys.argv[1:]
    
    parsed_args = parser.parse_args(args)
    
    # è®¾ç½®è¾“å‡ºçº§åˆ«
    if parsed_args.quiet:
        import logging
        logging.basicConfig(level=logging.WARNING)
    
    try:
        # ç”Ÿæˆæ•°æ®
        dataset = generate_complete_dataset(
            scale=parsed_args.scale,
            output_formats=parsed_args.format,
            output_dir=parsed_args.output_dir,
            validate_data=not parsed_args.no_validate,
            include_conflicts=not parsed_args.no_conflicts
        )
        
        if not parsed_args.quiet:
            print("\nğŸ¯ æ•°æ®ç”Ÿæˆä»»åŠ¡å®Œæˆï¼")
            print(f"   è§„æ¨¡: {parsed_args.scale}")
            print(f"   è®°å½•æ•°: {dataset['metadata']['total_records']:,}")
            print(f"   è¾“å‡ºç›®å½•: {parsed_args.output_dir}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        if not parsed_args.quiet:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
