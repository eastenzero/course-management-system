# file: data-generator/mega_scale/batch_manager.py
# åŠŸèƒ½: ç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆçš„æ‰¹å¤„ç†ç®¡ç†å™¨

import gc
import psutil
import time
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass
from pathlib import Path
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import json


@dataclass
class BatchConfig:
    """æ‰¹å¤„ç†é…ç½®"""
    batch_size: int = 50000           # æ¯æ‰¹æ•°æ®é‡
    max_memory_mb: int = 2048         # æœ€å¤§å†…å­˜é™åˆ¶(MB)
    max_workers: int = 4              # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
    checkpoint_interval: int = 100000  # æ£€æŸ¥ç‚¹é—´éš”
    gc_threshold: float = 0.8         # GCè§¦å‘é˜ˆå€¼
    enable_compression: bool = True    # å¯ç”¨å‹ç¼©
    enable_streaming: bool = True      # å¯ç”¨æµå¼å¤„ç†


@dataclass
class BatchStatus:
    """æ‰¹æ¬¡çŠ¶æ€"""
    batch_id: int
    total_records: int
    processed_records: int
    start_time: float
    estimated_finish_time: Optional[float] = None
    memory_usage_mb: float = 0.0
    error_count: int = 0
    status: str = "pending"  # pending, running, completed, failed


class DependencyGraph:
    """ä¾èµ–å…³ç³»å›¾ç®¡ç†å™¨"""
    
    def __init__(self):
        self.dependencies = {}
        self.reverse_dependencies = {}
    
    def add_dependency(self, dependent: str, dependency: str):
        """æ·»åŠ ä¾èµ–å…³ç³»: dependentä¾èµ–äºdependency"""
        if dependent not in self.dependencies:
            self.dependencies[dependent] = set()
        self.dependencies[dependent].add(dependency)
        
        if dependency not in self.reverse_dependencies:
            self.reverse_dependencies[dependency] = set()
        self.reverse_dependencies[dependency].add(dependent)
    
    def get_execution_order(self, tasks: List[str]) -> List[str]:
        """è·å–ä»»åŠ¡æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºï¼‰"""
        in_degree = {task: 0 for task in tasks}
        
        # è®¡ç®—å…¥åº¦
        for task in tasks:
            for dep in self.dependencies.get(task, []):
                if dep in in_degree:
                    in_degree[task] += 1
        
        # æ‹“æ‰‘æ’åº
        result = []
        queue = [task for task, degree in in_degree.items() if degree == 0]
        
        while queue:
            current = queue.pop(0)
            result.append(current)
            
            # æ›´æ–°ä¾èµ–è¯¥ä»»åŠ¡çš„å…¶ä»–ä»»åŠ¡
            for dependent in self.reverse_dependencies.get(current, []):
                if dependent in in_degree:
                    in_degree[dependent] -= 1
                    if in_degree[dependent] == 0:
                        queue.append(dependent)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¾ªç¯ä¾èµ–
        if len(result) != len(tasks):
            remaining = [task for task in tasks if task not in result]
            raise ValueError(f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {remaining}")
        
        return result


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self, max_memory_mb: int, gc_threshold: float = 0.8):
        self.max_memory_mb = max_memory_mb
        self.gc_threshold = gc_threshold
        self.process = psutil.Process()
    
    def get_memory_usage_mb(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def get_memory_percent(self) -> float:
        """è·å–å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”"""
        return self.get_memory_usage_mb() / self.max_memory_mb
    
    def should_trigger_gc(self) -> bool:
        """æ˜¯å¦åº”è¯¥è§¦å‘åƒåœ¾å›æ”¶"""
        return self.get_memory_percent() > self.gc_threshold
    
    def force_gc(self) -> float:
        """å¼ºåˆ¶åƒåœ¾å›æ”¶å¹¶è¿”å›å›æ”¶çš„å†…å­˜é‡(MB)"""
        before = self.get_memory_usage_mb()
        gc.collect()
        after = self.get_memory_usage_mb()
        return before - after


class BatchProcessingManager:
    """æ‰¹å¤„ç†ç®¡ç†å™¨"""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        self.dependency_graph = DependencyGraph()
        self.memory_monitor = MemoryMonitor(config.max_memory_mb, config.gc_threshold)
        self.batch_statuses: Dict[int, BatchStatus] = {}
        self.checkpoint_data = {}
        self.lock = threading.Lock()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_processed = 0
        self.start_time = None
        self.errors = []
    
    def calculate_optimal_batch_size(self, total_records: int, 
                                   estimated_record_size_bytes: int = 1024) -> int:
        """è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°"""
        # åŸºäºå†…å­˜é™åˆ¶è®¡ç®—
        max_memory_bytes = self.config.max_memory_mb * 1024 * 1024
        memory_based_batch_size = int(max_memory_bytes * 0.5 / estimated_record_size_bytes)
        
        # åŸºäºå¤„ç†å™¨æ ¸å¿ƒæ•°è°ƒæ•´
        cpu_count = psutil.cpu_count()
        cpu_based_batch_size = self.config.batch_size * cpu_count
        
        # åŸºäºæ€»è®°å½•æ•°è°ƒæ•´
        if total_records < 100000:
            size_based_batch_size = min(10000, total_records // 10)
        else:
            size_based_batch_size = min(self.config.batch_size, total_records // 20)
        
        # å–æœ€å°å€¼ä½œä¸ºæœ€ä¼˜å¤§å°
        optimal_size = min(
            memory_based_batch_size,
            cpu_based_batch_size,
            size_based_batch_size,
            self.config.batch_size
        )
        
        return max(1000, optimal_size)  # æœ€å°1000æ¡è®°å½•
    
    def create_batches(self, total_records: int, 
                      task_dependencies: Dict[str, List[str]] = None) -> List[Dict[str, Any]]:
        """åˆ›å»ºæ‰¹æ¬¡è®¡åˆ’"""
        batch_size = self.calculate_optimal_batch_size(total_records)
        total_batches = (total_records + batch_size - 1) // batch_size
        
        batches = []
        for i in range(total_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_records)
            
            batch = {
                'batch_id': i,
                'start_idx': start_idx,
                'end_idx': end_idx,
                'size': end_idx - start_idx,
                'dependencies': [],
                'status': 'pending'
            }
            batches.append(batch)
        
        # è®¾ç½®ä»»åŠ¡ä¾èµ–å…³ç³»
        if task_dependencies:
            for task, deps in task_dependencies.items():
                for dep in deps:
                    self.dependency_graph.add_dependency(task, dep)
        
        return batches
    
    def process_batch(self, batch_info: Dict[str, Any], 
                     processing_func: Callable, 
                     *args, **kwargs) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        batch_id = batch_info['batch_id']
        
        # åˆ›å»ºæ‰¹æ¬¡çŠ¶æ€
        status = BatchStatus(
            batch_id=batch_id,
            total_records=batch_info['size'],
            processed_records=0,
            start_time=time.time(),
            status="running"
        )
        
        with self.lock:
            self.batch_statuses[batch_id] = status
        
        try:
            # å†…å­˜æ£€æŸ¥
            if self.memory_monitor.should_trigger_gc():
                freed_mb = self.memory_monitor.force_gc()
                print(f"ğŸ§¹ æ‰¹æ¬¡ {batch_id}: æ‰§è¡Œåƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾ {freed_mb:.1f}MB")
            
            # æ‰§è¡Œå¤„ç†å‡½æ•°
            result = processing_func(
                batch_info['start_idx'], 
                batch_info['end_idx'],
                *args, **kwargs
            )
            
            # æ›´æ–°çŠ¶æ€
            with self.lock:
                status.processed_records = batch_info['size']
                status.memory_usage_mb = self.memory_monitor.get_memory_usage_mb()
                status.status = "completed"
                self.total_processed += batch_info['size']
            
            # æ£€æŸ¥ç‚¹ä¿å­˜
            if self.total_processed % self.config.checkpoint_interval == 0:
                self._save_checkpoint(batch_id, result)
            
            return {
                'batch_id': batch_id,
                'success': True,
                'result': result,
                'processing_time': time.time() - status.start_time,
                'memory_usage_mb': status.memory_usage_mb
            }
            
        except Exception as e:
            with self.lock:
                status.status = "failed"
                status.error_count += 1
                self.errors.append({
                    'batch_id': batch_id,
                    'error': str(e),
                    'timestamp': time.time()
                })
            
            return {
                'batch_id': batch_id,
                'success': False,
                'error': str(e),
                'processing_time': time.time() - status.start_time
            }
    
    def process_batches_parallel(self, batches: List[Dict[str, Any]], 
                               processing_func: Callable,
                               *args, **kwargs) -> List[Dict[str, Any]]:
        """å¹¶è¡Œå¤„ç†æ‰¹æ¬¡"""
        self.start_time = time.time()
        results = []
        
        # æ ¹æ®ä¾èµ–å…³ç³»ç¡®å®šæ‰§è¡Œé¡ºåº
        batch_names = [f"batch_{b['batch_id']}" for b in batches]
        execution_order = self.dependency_graph.get_execution_order(batch_names)
        
        # æŒ‰ä¾èµ–é¡ºåºåˆ›å»ºæ‰¹æ¬¡æ˜ å°„
        batch_map = {f"batch_{b['batch_id']}": b for b in batches}
        ordered_batches = [batch_map[name] for name in execution_order]
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {
                executor.submit(
                    self.process_batch, batch, processing_func, *args, **kwargs
                ): batch for batch in ordered_batches
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    # æ‰“å°è¿›åº¦
                    success_count = sum(1 for r in results if r['success'])
                    total_batches = len(batches)
                    progress = len(results) / total_batches * 100
                    
                    print(f"ğŸ“¦ æ‰¹æ¬¡ {result['batch_id']:3d}/{total_batches} "
                          f"{'âœ…' if result['success'] else 'âŒ'} "
                          f"è¿›åº¦: {progress:5.1f}% "
                          f"å†…å­˜: {result.get('memory_usage_mb', 0):.0f}MB")
                    
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {e}")
                    results.append({
                        'batch_id': batch['batch_id'],
                        'success': False,
                        'error': str(e)
                    })
        
        return results
    
    def _save_checkpoint(self, batch_id: int, data: Any):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path("checkpoints")
        checkpoint_dir.mkdir(exist_ok=True)
        
        checkpoint_file = checkpoint_dir / f"checkpoint_batch_{batch_id}.json"
        
        checkpoint_data = {
            'batch_id': batch_id,
            'timestamp': time.time(),
            'total_processed': self.total_processed,
            'memory_usage_mb': self.memory_monitor.get_memory_usage_mb(),
            'data_summary': {
                'type': type(data).__name__,
                'size': len(data) if hasattr(data, '__len__') else 'unknown'
            }
        }
        
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
    
    def get_progress_summary(self) -> Dict[str, Any]:
        """è·å–è¿›åº¦æ‘˜è¦"""
        with self.lock:
            total_batches = len(self.batch_statuses)
            completed = sum(1 for s in self.batch_statuses.values() if s.status == "completed")
            failed = sum(1 for s in self.batch_statuses.values() if s.status == "failed")
            running = sum(1 for s in self.batch_statuses.values() if s.status == "running")
            
            elapsed_time = time.time() - (self.start_time or time.time())
            
            # è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´
            if completed > 0 and elapsed_time > 0:
                avg_time_per_batch = elapsed_time / completed
                remaining_batches = total_batches - completed
                estimated_remaining_time = avg_time_per_batch * remaining_batches
            else:
                estimated_remaining_time = None
            
            return {
                'total_batches': total_batches,
                'completed': completed,
                'failed': failed,
                'running': running,
                'pending': total_batches - completed - failed - running,
                'total_processed': self.total_processed,
                'progress_percent': completed / total_batches * 100 if total_batches > 0 else 0,
                'elapsed_time_seconds': elapsed_time,
                'estimated_remaining_seconds': estimated_remaining_time,
                'current_memory_mb': self.memory_monitor.get_memory_usage_mb(),
                'memory_usage_percent': self.memory_monitor.get_memory_percent() * 100,
                'error_count': len(self.errors),
                'processing_speed_records_per_second': self.total_processed / elapsed_time if elapsed_time > 0 else 0
            }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        self.memory_monitor.force_gc()
        
        # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶
        checkpoint_dir = Path("checkpoints")
        if checkpoint_dir.exists():
            for file in checkpoint_dir.glob("checkpoint_batch_*.json"):
                try:
                    file.unlink()
                except:
                    pass