# file: data-generator/mega_scale/mega_generator.py
# åŠŸèƒ½: ç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆä¸»æ§åˆ¶å™¨

import sys
import time
import json
import threading
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from .batch_manager import BatchProcessingManager, BatchConfig
from .memory_optimizer import MemoryOptimizer, StreamConfig
from .parallel_engine import ParallelComputingEngine, TaskConfig, TaskResult
from .progress_monitor import ProgressMonitor

# å¯¼å…¥åŸæœ‰çš„ç”Ÿæˆå™¨
from config import DATA_SCALE_CONFIG
from generators import (
    DepartmentGenerator,
    UserGenerator,
    CourseGenerator,
    FacilityGenerator,
    DataExporter
)
from generators.realistic_constraints import RealisticConstraintsEngine
from generators.relationship_modeling import RelationshipModelingEngine
from generators.conflict_generator import ConflictGeneratorEngine
from generators.quality_assessment import DataQualityAssessment


@dataclass
class MegaGenerationConfig:
    """ç™¾ä¸‡çº§ç”Ÿæˆé…ç½®"""
    target_records: int = 1000000     # ç›®æ ‡è®°å½•æ•°
    batch_size: int = 50000           # æ‰¹æ¬¡å¤§å°
    max_memory_mb: int = 2048         # æœ€å¤§å†…å­˜é™åˆ¶
    max_workers: int = 8              # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
    enable_compression: bool = True    # å¯ç”¨å‹ç¼©
    enable_streaming: bool = True      # å¯ç”¨æµå¼å†™å…¥
    enable_checkpoints: bool = True    # å¯ç”¨æ£€æŸ¥ç‚¹
    output_formats: List[str] = None   # è¾“å‡ºæ ¼å¼
    
    def __post_init__(self):
        if self.output_formats is None:
            self.output_formats = ['json']


class MegaDataGenerator:
    """ç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, config: MegaGenerationConfig = None):
        self.config = config or MegaGenerationConfig()
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.batch_manager = BatchProcessingManager(BatchConfig(
            batch_size=self.config.batch_size,
            max_memory_mb=self.config.max_memory_mb,
            max_workers=self.config.max_workers,
            enable_compression=self.config.enable_compression,
            enable_streaming=self.config.enable_streaming
        ))
        
        self.memory_optimizer = MemoryOptimizer(self.config.max_memory_mb)
        
        self.parallel_engine = ParallelComputingEngine(self.config.max_workers)
        
        self.progress_monitor = ProgressMonitor(self.config.target_records)
        
        # åŸæœ‰ç”Ÿæˆå™¨ç»„ä»¶
        self.realistic_engine = RealisticConstraintsEngine()
        self.relationship_engine = RelationshipModelingEngine()
        self.conflict_engine = ConflictGeneratorEngine()
        self.quality_assessor = DataQualityAssessment()
        
        # çŠ¶æ€è·Ÿè¸ª
        self.generation_started = False
        self.generation_completed = False
        self.start_time: Optional[float] = None
        self.end_time: Optional[float] = None
        
        # ç»“æœå­˜å‚¨
        self.final_results: Dict[str, Any] = {}
        self.performance_stats: Dict[str, Any] = {}
    
    def generate_mega_dataset(self, scale: str = 'huge', 
                            output_dir: str = 'mega_output',
                            conflict_difficulty: str = 'mixed') -> Dict[str, Any]:
        """ç”Ÿæˆç™¾ä¸‡çº§æ•°æ®é›†"""
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹ç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆç³»ç»Ÿ")
        print(f"ğŸ“Š ç›®æ ‡è§„æ¨¡: {self.config.target_records:,} æ¡è®°å½•")
        print(f"ğŸ”§ æ‰¹æ¬¡å¤§å°: {self.config.batch_size:,}")
        print(f"ğŸ’¾ å†…å­˜é™åˆ¶: {self.config.max_memory_mb}MB")
        print(f"âš¡ å¹¶è¡Œåº¦: {self.config.max_workers} å·¥ä½œè¿›ç¨‹")
        print(f"{'='*80}")
        
        self.generation_started = True
        self.start_time = time.time()
        
        try:
            # 1. ç³»ç»Ÿåˆå§‹åŒ–å’Œä¼˜åŒ–
            self._initialize_system()
            
            # 2. å¯åŠ¨ç›‘æ§
            self.progress_monitor.start_monitoring(enable_progress_bar=True)
            
            # 3. ç”ŸæˆåŸºç¡€æ•°æ®
            print("\nğŸ“š é˜¶æ®µ1: ç”ŸæˆåŸºç¡€æ•°æ®...")
            basic_dataset = self._generate_basic_data_mega(scale)
            
            # 4. åº”ç”¨çœŸå®æ€§çº¦æŸ
            print("\nğŸ¯ é˜¶æ®µ2: åº”ç”¨çœŸå®æ€§çº¦æŸ...")
            enhanced_dataset = self._apply_realistic_constraints_mega(basic_dataset)
            
            # 5. æ„å»ºå…³è”æ€§æ¨¡å‹
            print("\nğŸ”— é˜¶æ®µ3: æ„å»ºå…³è”æ€§æ¨¡å‹...")
            modeled_dataset = self._build_relationship_model_mega(enhanced_dataset)
            
            # 6. ç”Ÿæˆå†²çªåœºæ™¯
            print("\nâš¡ é˜¶æ®µ4: ç”Ÿæˆå†²çªåœºæ™¯...")
            conflict_dataset = self._generate_conflicts_mega(modeled_dataset, conflict_difficulty)
            
            # 7. è´¨é‡è¯„ä¼°
            print("\nğŸ“Š é˜¶æ®µ5: è´¨é‡è¯„ä¼°...")
            quality_report = self._assess_quality_mega(conflict_dataset)
            conflict_dataset['quality_report'] = quality_report
            
            # 8. æ•°æ®å¯¼å‡º
            print("\nğŸ’¾ é˜¶æ®µ6: æ•°æ®å¯¼å‡º...")
            self._export_data_mega(conflict_dataset, output_dir)
            
            # 9. æœ€ç»ˆç»Ÿè®¡
            self._finalize_generation(conflict_dataset)
            
            return self.final_results
            
        except Exception as e:
            error_id = self.progress_monitor.handle_error(e, {
                'stage': 'mega_generation',
                'config': self.config.__dict__
            })
            print(f"âŒ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            print(f"ğŸ” é”™è¯¯ID: {error_id}")
            raise
        
        finally:
            self._cleanup_resources()
    
    def _initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("âš™ï¸ åˆå§‹åŒ–ç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–å¹¶è¡Œå¼•æ“
        self.parallel_engine.initialize_workers(
            process_workers=max(1, self.config.max_workers // 2),
            thread_workers=max(1, self.config.max_workers // 2)
        )
        
        # å¯åŠ¨å¹¶è¡Œå¼•æ“
        self.parallel_engine.start_processing()
        
        # å†…å­˜ä¼˜åŒ–è®¾ç½®
        optimization_result = self.memory_optimizer.optimize_for_large_scale(
            self.config.target_records
        )
        
        print(f"   âœ… å¹¶è¡Œå¼•æ“: {self.config.max_workers} å·¥ä½œè¿›ç¨‹")
        print(f"   âœ… å†…å­˜ä¼˜åŒ–: {len(optimization_result['optimizations_applied'])} é¡¹ä¼˜åŒ–")
        print(f"   âœ… æ‰¹æ¬¡é…ç½®: {self.config.batch_size:,} è®°å½•/æ‰¹æ¬¡")
        
        # æ³¨å†Œä»»åŠ¡å¤„ç†å‡½æ•°
        self._register_task_functions()
    
    def _register_task_functions(self):
        """æ³¨å†Œä»»åŠ¡å¤„ç†å‡½æ•°"""
        self.parallel_engine.register_task_function('generate_students', self._generate_students_batch)
        self.parallel_engine.register_task_function('generate_teachers', self._generate_teachers_batch)
        self.parallel_engine.register_task_function('generate_courses', self._generate_courses_batch)
        self.parallel_engine.register_task_function('apply_constraints', self._apply_constraints_batch)
        self.parallel_engine.register_task_function('build_relationships', self._build_relationships_batch)
        self.parallel_engine.register_task_function('generate_conflicts', self._generate_conflicts_batch)
    
    def _generate_basic_data_mega(self, scale: str) -> Dict[str, Any]:
        """å¤§è§„æ¨¡ç”ŸæˆåŸºç¡€æ•°æ®"""
        config = DATA_SCALE_CONFIG[scale]
        
        # ä½¿ç”¨æ‰¹å¤„ç†ç®¡ç†å™¨åˆ›å»ºæ‰¹æ¬¡
        total_student_records = config['students']
        student_batches = self.batch_manager.create_batches(total_student_records)
        
        print(f"   ğŸ“Š è§„åˆ’ {len(student_batches)} ä¸ªå­¦ç”Ÿæ‰¹æ¬¡")
        
        # ç”Ÿæˆé™¢ç³»ã€ä¸“ä¸šç­‰åŸºç¡€æ•°æ®ï¼ˆè¿™äº›æ•°æ®é‡è¾ƒå°ï¼Œç›´æ¥ç”Ÿæˆï¼‰
        print("   ğŸ¢ ç”Ÿæˆé™¢ç³»å’Œä¸“ä¸š...")
        dept_gen = DepartmentGenerator()
        departments = dept_gen.generate_departments(config['departments'])
        majors = dept_gen.generate_majors(departments)
        
        print("   ğŸ« ç”Ÿæˆæ•™å®¤å’Œæ—¶é—´æ®µ...")
        facility_gen = FacilityGenerator()
        classrooms = facility_gen.generate_classrooms(config['classrooms'])
        time_slots = facility_gen.generate_time_slots()
        
        # å¹¶è¡Œç”Ÿæˆå¤§é‡æ•°æ®
        print("   ğŸ‘¥ å¹¶è¡Œç”Ÿæˆå­¦ç”Ÿæ•°æ®...")
        students = self._generate_students_parallel(config['students'], majors)
        
        print("   ğŸ‘¨â€ğŸ« å¹¶è¡Œç”Ÿæˆæ•™å¸ˆæ•°æ®...")
        teachers = self._generate_teachers_parallel(config['teachers'], departments)
        
        print("   ğŸ“š å¹¶è¡Œç”Ÿæˆè¯¾ç¨‹æ•°æ®...")
        courses = self._generate_courses_parallel(config['courses'], departments, teachers)
        
        # æ›´æ–°è¿›åº¦
        total_generated = len(students) + len(teachers) + len(courses)
        self.progress_monitor.update_progress(total_generated)
        
        return {
            'departments': departments,
            'majors': majors,
            'students': students,
            'teachers': teachers,
            'courses': courses,
            'classrooms': classrooms,
            'time_slots': time_slots,
            'metadata': {
                'scale': config,
                'generation_stage': 'basic_mega',
                'total_records': total_generated
            }
        }
    
    def _generate_students_parallel(self, total_students: int, majors: List[Dict]) -> List[Dict]:
        """å¹¶è¡Œç”Ÿæˆå­¦ç”Ÿæ•°æ®"""
        # åˆ›å»ºæ‰¹æ¬¡ä»»åŠ¡
        batch_size = self.config.batch_size
        num_batches = (total_students + batch_size - 1) // batch_size
        
        task_configs = []
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_students)
            
            task_config = TaskConfig(
                task_id=f"student_batch_{i}",
                task_type="generate_students",
                priority=5,
                estimated_duration=30.0,
                memory_requirement_mb=100
            )
            task_configs.append((task_config, self._generate_students_batch, (start_idx, end_idx, majors), {}))
        
        # æäº¤ä»»åŠ¡
        task_ids = self.parallel_engine.submit_batch_tasks(task_configs)
        
        # ç­‰å¾…å®Œæˆå¹¶åˆå¹¶ç»“æœ
        self.parallel_engine.wait_for_completion()
        results = self.parallel_engine.get_results()
        
        # åˆå¹¶å­¦ç”Ÿæ•°æ®
        all_students = []
        for task_id in task_ids:
            if task_id in results and results[task_id].success:
                batch_students = results[task_id].result
                all_students.extend(batch_students)
        
        return all_students
    
    def _generate_teachers_parallel(self, total_teachers: int, departments: List[Dict]) -> List[Dict]:
        """å¹¶è¡Œç”Ÿæˆæ•™å¸ˆæ•°æ®"""
        # æ•™å¸ˆæ•°é‡ç›¸å¯¹è¾ƒå°‘ï¼Œå¯ä»¥ç”¨è¾ƒå°çš„æ‰¹æ¬¡
        batch_size = min(5000, self.config.batch_size // 10)
        num_batches = (total_teachers + batch_size - 1) // batch_size
        
        task_configs = []
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_teachers)
            
            task_config = TaskConfig(
                task_id=f"teacher_batch_{i}",
                task_type="generate_teachers",
                priority=5,
                estimated_duration=15.0,
                memory_requirement_mb=50
            )
            task_configs.append((task_config, self._generate_teachers_batch, (start_idx, end_idx, departments), {}))
        
        # æäº¤å¹¶æ‰§è¡Œä»»åŠ¡
        task_ids = self.parallel_engine.submit_batch_tasks(task_configs)
        self.parallel_engine.wait_for_completion()
        results = self.parallel_engine.get_results()
        
        # åˆå¹¶æ•™å¸ˆæ•°æ®
        all_teachers = []
        for task_id in task_ids:
            if task_id in results and results[task_id].success:
                batch_teachers = results[task_id].result
                all_teachers.extend(batch_teachers)
        
        return all_teachers
    
    def _generate_courses_parallel(self, total_courses: int, departments: List[Dict], teachers: List[Dict]) -> List[Dict]:
        """å¹¶è¡Œç”Ÿæˆè¯¾ç¨‹æ•°æ®"""
        batch_size = min(10000, self.config.batch_size // 5)
        num_batches = (total_courses + batch_size - 1) // batch_size
        
        task_configs = []
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_courses)
            
            task_config = TaskConfig(
                task_id=f"course_batch_{i}",
                task_type="generate_courses",
                priority=5,
                estimated_duration=20.0,
                memory_requirement_mb=75
            )
            task_configs.append((task_config, self._generate_courses_batch, (start_idx, end_idx, departments, teachers), {}))
        
        # æäº¤å¹¶æ‰§è¡Œä»»åŠ¡
        task_ids = self.parallel_engine.submit_batch_tasks(task_configs)
        self.parallel_engine.wait_for_completion()
        results = self.parallel_engine.get_results()
        
        # åˆå¹¶è¯¾ç¨‹æ•°æ®
        all_courses = []
        for task_id in task_ids:
            if task_id in results and results[task_id].success:
                batch_courses = results[task_id].result
                all_courses.extend(batch_courses)
        
        return all_courses
    
    def _generate_students_batch(self, start_idx: int, end_idx: int, majors: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹å­¦ç”Ÿæ•°æ®"""
        user_gen = UserGenerator()
        batch_size = end_idx - start_idx
        
        students = []
        for i in range(batch_size):
            student_id = start_idx + i + 1
            student = user_gen.generate_student(student_id, majors)
            students.append(student)
        
        return students
    
    def _generate_teachers_batch(self, start_idx: int, end_idx: int, departments: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹æ•™å¸ˆæ•°æ®"""
        user_gen = UserGenerator()
        batch_size = end_idx - start_idx
        
        teachers = []
        for i in range(batch_size):
            teacher_id = start_idx + i + 1
            teacher = user_gen.generate_teacher(teacher_id, departments)
            teachers.append(teacher)
        
        return teachers
    
    def _generate_courses_batch(self, start_idx: int, end_idx: int, 
                               departments: List[Dict], teachers: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹è¯¾ç¨‹æ•°æ®"""
        course_gen = CourseGenerator()
        batch_size = end_idx - start_idx
        
        courses = []
        for i in range(batch_size):
            course = course_gen.generate_course(departments, teachers)
            courses.append(course)
        
        return courses
    
    def _apply_constraints_batch(self, *args, **kwargs) -> List[Dict]:
        """åº”ç”¨çº¦æŸæ¡ä»¶æ‰¹å¤„ç†"""
        # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦æ–¹æ³•ï¼Œç”¨äºçº¦æŸå¤„ç†
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å„ç§çº¦æŸæ¡ä»¶
        return []
    
    def _build_relationships_batch(self, *args, **kwargs) -> List[Dict]:
        """æ„å»ºå…³ç³»æ‰¹å¤„ç†"""
        # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦æ–¹æ³•ï¼Œç”¨äºå…³ç³»æ„å»º
        return []
    
    def _generate_conflicts_batch(self, *args, **kwargs) -> List[Dict]:
        """ç”Ÿæˆå†²çªæ‰¹å¤„ç†"""
        # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦æ–¹æ³•ï¼Œç”¨äºå†²çªç”Ÿæˆ
        return []
    
    def _apply_realistic_constraints_mega(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """å¤§è§„æ¨¡åº”ç”¨çœŸå®æ€§çº¦æŸ"""
        print("   âš™ï¸ åº”ç”¨çœŸå®æ€§çº¦æŸ...")
        
        enhanced_dataset = dataset.copy()
        
        # åˆ†æ‰¹å¤„ç†æ•™å¸ˆåå¥½
        print("   ğŸ“‹ ç”Ÿæˆæ•™å¸ˆåå¥½...")
        teacher_preferences = []
        batch_size = 5000
        
        for i in range(0, len(dataset['teachers']), batch_size):
            batch_teachers = dataset['teachers'][i:i+batch_size]
            
            for teacher in batch_teachers:
                prefs = self.realistic_engine.generate_realistic_teacher_preferences(teacher)
                teacher_preferences.append(prefs)
            
            # å¢é‡å†™å…¥
            if self.config.enable_streaming:
                self.memory_optimizer.write_incrementally(
                    'mega_output/teacher_preferences_batch.json',
                    teacher_preferences[-len(batch_teachers):],
                    'json'
                )
            
            # æ›´æ–°è¿›åº¦
            self.progress_monitor.update_progress(
                self.progress_monitor.current_metrics.processed_records + len(batch_teachers)
            )
        
        enhanced_dataset['teacher_preferences'] = teacher_preferences
        
        # å¢å¼ºè¯¾ç¨‹çœŸå®æ€§
        print("   ğŸ“š å¢å¼ºè¯¾ç¨‹çœŸå®æ€§...")
        enhanced_courses = self.realistic_engine.generate_realistic_course_distribution(
            dataset['courses'], dataset['departments']
        )
        enhanced_dataset['courses'] = enhanced_courses
        
        # ç”Ÿæˆé€‰è¯¾æ¨¡å¼
        print("   ğŸ“ ç”Ÿæˆé€‰è¯¾æ¨¡å¼...")
        enrollments = self.realistic_engine.generate_realistic_student_enrollment_patterns(
            dataset['students'], enhanced_courses
        )
        enhanced_dataset['enrollments'] = enrollments
        
        enhanced_dataset['metadata']['generation_stage'] = 'realistic_enhanced_mega'
        return enhanced_dataset
    
    def _build_relationship_model_mega(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """å¤§è§„æ¨¡æ„å»ºå…³è”æ€§æ¨¡å‹"""
        print("   ğŸ”— æ„å»ºå…³è”æ€§æ¨¡å‹...")
        
        modeled_dataset = dataset.copy()
        
        # ç”Ÿæˆè¯¾ç¨‹ä¾èµ–å…³ç³»
        print("   ğŸ“– ç”Ÿæˆè¯¾ç¨‹ä¾èµ–å…³ç³»...")
        dependencies = self.relationship_engine.generate_course_dependencies(dataset['courses'])
        modeled_dataset['course_dependencies'] = dependencies
        
        # åˆ†æ‰¹ç”Ÿæˆæ•™å¸ˆèƒ½åŠ›æ¡£æ¡ˆ
        print("   ğŸ‘¨â€ğŸ« ç”Ÿæˆæ•™å¸ˆèƒ½åŠ›æ¡£æ¡ˆ...")
        teacher_competencies = []
        batch_size = 1000
        
        for i in range(0, len(dataset['teachers']), batch_size):
            batch_teachers = dataset['teachers'][i:i+batch_size]
            batch_competencies = self.relationship_engine.generate_teacher_competency_profiles(
                batch_teachers, dataset['departments']
            )
            teacher_competencies.extend(batch_competencies)
            
            # å®šæœŸè§¦å‘å†…å­˜ä¼˜åŒ–
            self.memory_optimizer.trigger_gc_if_needed()
        
        modeled_dataset['teacher_competencies'] = teacher_competencies
        
        # ä¼˜åŒ–æ•™å¸ˆè¯¾ç¨‹åˆ†é…
        print("   ğŸ¯ ä¼˜åŒ–æ•™å¸ˆè¯¾ç¨‹åˆ†é…...")
        optimized_assignments = self.relationship_engine.optimize_teacher_course_assignments(
            dataset['courses'], dataset['teachers']
        )
        modeled_dataset['optimized_assignments'] = optimized_assignments
        
        modeled_dataset['metadata']['generation_stage'] = 'relationship_modeled_mega'
        return modeled_dataset
    
    def _generate_conflicts_mega(self, dataset: Dict[str, Any], difficulty: str) -> Dict[str, Any]:
        """å¤§è§„æ¨¡ç”Ÿæˆå†²çªåœºæ™¯"""
        print("   âš¡ ç”Ÿæˆå†²çªåœºæ™¯...")
        
        conflict_dataset = dataset.copy()
        
        # ç”Ÿæˆå†²çªåœºæ™¯
        print(f"   âš™ï¸ ç”Ÿæˆ{difficulty}çº§åˆ«å†²çªåœºæ™¯...")
        conflict_scenarios = self.conflict_engine.generate_conflict_scenarios(
            dataset['courses'], 
            dataset['teachers'], 
            dataset['classrooms'],
            target_difficulty=difficulty
        )
        conflict_dataset['conflicts'] = conflict_scenarios
        
        # ç”Ÿæˆå†²çªç»Ÿè®¡
        print("   ğŸ“Š ç”Ÿæˆå†²çªç»Ÿè®¡...")
        conflict_stats = self.conflict_engine.generate_conflict_statistics()
        conflict_dataset['conflict_statistics'] = conflict_stats
        
        conflict_dataset['metadata']['generation_stage'] = 'conflict_enhanced_mega'
        conflict_dataset['metadata']['conflict_difficulty'] = difficulty
        
        return conflict_dataset
    
    def _assess_quality_mega(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """å¤§è§„æ¨¡è´¨é‡è¯„ä¼°"""
        print("   ğŸ“Š è¿›è¡Œè´¨é‡è¯„ä¼°...")
        
        # é‡‡æ ·è¯„ä¼°ä»¥æé«˜æ€§èƒ½
        sample_size = min(10000, len(dataset.get('students', [])) // 100)
        
        print(f"   ğŸ” é‡‡æ ·è¯„ä¼° (æ ·æœ¬å¤§å°: {sample_size:,})")
        
        quality_report = self.quality_assessor.generate_quality_report(
            dataset, sample_size=sample_size
        )
        
        return quality_report
    
    def _export_data_mega(self, dataset: Dict[str, Any], output_dir: str):
        """å¤§è§„æ¨¡æ•°æ®å¯¼å‡º"""
        print(f"   ğŸ’¾ å¯¼å‡ºæ•°æ®åˆ° {output_dir}...")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # åˆ†åˆ«å¯¼å‡ºå„ç±»æ•°æ®
        exporter = DataExporter(str(output_path))
        
        for data_type in ['students', 'teachers', 'courses', 'enrollments']:
            if data_type in dataset:
                data = dataset[data_type]
                
                print(f"   ğŸ“ å¯¼å‡º {data_type}: {len(data):,} æ¡è®°å½•")
                
                # åˆ†æ–‡ä»¶å¯¼å‡ºå¤§æ•°æ®
                if len(data) > 100000:
                    self._export_large_data(data, data_type, output_path)
                else:
                    exporter.export_to_json(data, f'{data_type}.json')
        
        # å¯¼å‡ºå…ƒæ•°æ®å’ŒæŠ¥å‘Š
        exporter.export_to_json(dataset['metadata'], 'metadata.json')
        if 'quality_report' in dataset:
            exporter.export_to_json(dataset['quality_report'], 'quality_report.json')
    
    def _export_large_data(self, data: List[Dict], data_type: str, output_path: Path):
        """å¯¼å‡ºå¤§å‹æ•°æ®é›†"""
        chunk_size = 50000
        
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i+chunk_size]
            chunk_file = output_path / f"{data_type}_part_{i//chunk_size + 1:03d}.json"
            
            if self.config.enable_compression:
                chunk_file = chunk_file.with_suffix('.json.gz')
            
            # ä½¿ç”¨æµå¼å†™å…¥
            for record in chunk:
                self.memory_optimizer.write_incrementally(str(chunk_file), record, 'json')
    
    def _finalize_generation(self, dataset: Dict[str, Any]):
        """å®Œæˆç”Ÿæˆè¿‡ç¨‹"""
        self.end_time = time.time()
        self.generation_completed = True
        
        # æ”¶é›†æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_time': self.end_time - self.start_time,
            'total_records': sum(len(dataset.get(key, [])) for key in ['students', 'teachers', 'courses']),
            'memory_stats': self.memory_optimizer.get_optimization_stats(),
            'parallel_stats': self.parallel_engine.get_performance_stats(),
            'progress_report': self.progress_monitor.get_status_report()
        }
        
        # å­˜å‚¨æœ€ç»ˆç»“æœ
        self.final_results = {
            'dataset': dataset,
            'performance_stats': self.performance_stats,
            'config': self.config.__dict__,
            'success': True
        }
        
        # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
        self._print_final_report()
    
    def _print_final_report(self):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        stats = self.performance_stats
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        print(f"{'='*80}")
        print(f"â±ï¸  æ€»è€—æ—¶: {stats['total_time']:.1f} ç§’")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {stats['total_records']:,}")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {stats['total_records']/stats['total_time']:.0f} æ¡/ç§’")
        print(f"ğŸ’¾ å³°å€¼å†…å­˜: {stats['memory_stats']['peak_memory_mb']:.0f}MB")
        print(f"ğŸ§¹ GCæ¬¡æ•°: {stats['memory_stats']['gc_count']}")
        print(f"âš¡ å¹¶è¡Œæ•ˆç‡: {stats['parallel_stats']['parallel_efficiency']:.1f}%")
        
        errors = stats['progress_report']['errors']
        if errors['total_errors'] > 0:
            print(f"âŒ æ€»é”™è¯¯æ•°: {errors['total_errors']} (è§£å†³ç‡: {errors['resolution_rate']:.1f}%)")
        
        print(f"{'='*80}")
    
    def _cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº...")
        
        # åœæ­¢ç›‘æ§
        self.progress_monitor.stop_monitoring()
        
        # åœæ­¢å¹¶è¡Œå¼•æ“
        self.parallel_engine.stop()
        
        # æ¸…ç†å†…å­˜ä¼˜åŒ–å™¨
        self.memory_optimizer.cleanup()
        
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    
    # é…ç½®ç™¾ä¸‡çº§ç”Ÿæˆå‚æ•°
    config = MegaGenerationConfig(
        target_records=1000000,
        batch_size=50000,
        max_memory_mb=2048,
        max_workers=8,
        enable_compression=True,
        enable_streaming=True,
        output_formats=['json']
    )
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = MegaDataGenerator(config)
    
    try:
        # å¼€å§‹ç”Ÿæˆ
        results = generator.generate_mega_dataset(
            scale='huge',
            output_dir='mega_output',
            conflict_difficulty='mixed'
        )
        
        print("ğŸŠ ç™¾ä¸‡çº§æ•°æ®ç”ŸæˆæˆåŠŸå®Œæˆï¼")
        return results
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    main()