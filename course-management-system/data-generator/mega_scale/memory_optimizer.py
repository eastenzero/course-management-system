# file: data-generator/mega_scale/memory_optimizer.py
# åŠŸèƒ½: å†…å­˜ä¼˜åŒ–æ¨¡å—å’Œæµå¼å†™å…¥æœºåˆ¶

import gc
import gzip
import json
import threading
import weakref
from io import StringIO
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Iterator, TextIO
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import psutil
import time
from dataclasses import dataclass, field


@dataclass
class MemoryPool:
    """å†…å­˜æ± é…ç½®"""
    max_objects: int = 10000          # æœ€å¤§å¯¹è±¡æ•°
    cleanup_threshold: float = 0.8    # æ¸…ç†é˜ˆå€¼
    object_lifetime: float = 300.0    # å¯¹è±¡ç”Ÿå­˜æ—¶é—´(ç§’)


@dataclass
class StreamConfig:
    """æµå¼å¤„ç†é…ç½®"""
    buffer_size: int = 64 * 1024      # å†™å…¥ç¼“å†²åŒºå¤§å° (64KB)
    compression_level: int = 6         # å‹ç¼©çº§åˆ« (1-9)
    max_file_size_mb: int = 500       # å•æ–‡ä»¶æœ€å¤§å¤§å° (MB)
    enable_async_write: bool = True    # å¼‚æ­¥å†™å…¥
    write_queue_size: int = 1000      # å†™å…¥é˜Ÿåˆ—å¤§å°


class ObjectPool:
    """å¯¹è±¡æ± ç®¡ç†å™¨"""
    
    def __init__(self, config: MemoryPool):
        self.config = config
        self.pools: Dict[str, List[Any]] = {}
        self.object_timestamps: Dict[id, float] = {}
        self.lock = threading.Lock()
        self.weak_refs: Dict[str, List[weakref.ref]] = {}
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self.cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)
        self.cleanup_thread.start()
    
    def get_object(self, object_type: str, factory_func: callable) -> Any:
        """è·å–å¯¹è±¡"""
        with self.lock:
            if object_type not in self.pools:
                self.pools[object_type] = []
                self.weak_refs[object_type] = []
            
            # å°è¯•ä»æ± ä¸­è·å–å¯¹è±¡
            pool = self.pools[object_type]
            if pool:
                obj = pool.pop()
                # é‡ç½®å¯¹è±¡æ—¶é—´æˆ³
                self.object_timestamps[id(obj)] = time.time()
                return obj
            
            # åˆ›å»ºæ–°å¯¹è±¡
            obj = factory_func()
            self.object_timestamps[id(obj)] = time.time()
            
            # æ·»åŠ å¼±å¼•ç”¨ç”¨äºè¿½è¸ª
            weak_ref = weakref.ref(obj, lambda ref: self._on_object_deleted(ref, object_type))
            self.weak_refs[object_type].append(weak_ref)
            
            return obj
    
    def return_object(self, object_type: str, obj: Any):
        """å½’è¿˜å¯¹è±¡"""
        with self.lock:
            if object_type not in self.pools:
                return
            
            pool = self.pools[object_type]
            if len(pool) < self.config.max_objects:
                # æ¸…ç†å¯¹è±¡çŠ¶æ€
                self._reset_object(obj)
                pool.append(obj)
                self.object_timestamps[id(obj)] = time.time()
    
    def _reset_object(self, obj: Any):
        """é‡ç½®å¯¹è±¡çŠ¶æ€"""
        if hasattr(obj, 'clear'):
            obj.clear()
        elif hasattr(obj, 'reset'):
            obj.reset()
        elif isinstance(obj, dict):
            obj.clear()
        elif isinstance(obj, list):
            obj.clear()
    
    def _on_object_deleted(self, weak_ref: weakref.ref, object_type: str):
        """å¯¹è±¡è¢«åˆ é™¤æ—¶çš„å›è°ƒ"""
        with self.lock:
            if object_type in self.weak_refs:
                try:
                    self.weak_refs[object_type].remove(weak_ref)
                except ValueError:
                    pass
    
    def _cleanup_worker(self):
        """æ¸…ç†å·¥ä½œçº¿ç¨‹"""
        while True:
            try:
                time.sleep(60)  # æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                self._cleanup_expired_objects()
            except Exception as e:
                print(f"å¯¹è±¡æ± æ¸…ç†å¼‚å¸¸: {e}")
    
    def _cleanup_expired_objects(self):
        """æ¸…ç†è¿‡æœŸå¯¹è±¡"""
        current_time = time.time()
        
        with self.lock:
            total_objects = sum(len(pool) for pool in self.pools.values())
            if total_objects < self.config.max_objects * self.config.cleanup_threshold:
                return
            
            for object_type, pool in self.pools.items():
                expired_objects = []
                
                for obj in pool[:]:
                    obj_id = id(obj)
                    if obj_id in self.object_timestamps:
                        age = current_time - self.object_timestamps[obj_id]
                        if age > self.config.object_lifetime:
                            expired_objects.append(obj)
                
                # ç§»é™¤è¿‡æœŸå¯¹è±¡
                for obj in expired_objects:
                    try:
                        pool.remove(obj)
                        obj_id = id(obj)
                        if obj_id in self.object_timestamps:
                            del self.object_timestamps[obj_id]
                    except ValueError:
                        pass
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            if total_objects > self.config.max_objects * 0.9:
                gc.collect()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            stats = {
                'total_pools': len(self.pools),
                'pools': {}
            }
            
            for object_type, pool in self.pools.items():
                stats['pools'][object_type] = {
                    'size': len(pool),
                    'weak_refs': len(self.weak_refs.get(object_type, []))
                }
            
            stats['total_objects'] = sum(len(pool) for pool in self.pools.values())
            stats['total_timestamps'] = len(self.object_timestamps)
            
            return stats


class StreamWriter:
    """æµå¼å†™å…¥å™¨"""
    
    def __init__(self, config: StreamConfig):
        self.config = config
        self.write_queue: Queue = Queue(maxsize=config.write_queue_size)
        self.writers: Dict[str, Any] = {}
        self.file_sizes: Dict[str, int] = {}
        self.file_counters: Dict[str, int] = {}
        self.lock = threading.Lock()
        
        # å¯åŠ¨å¼‚æ­¥å†™å…¥çº¿ç¨‹
        if config.enable_async_write:
            self.write_thread = threading.Thread(target=self._write_worker, daemon=True)
            self.write_thread.start()
        else:
            self.write_thread = None
    
    def write_data(self, file_path: str, data: Any, format_type: str = 'json'):
        """å†™å…¥æ•°æ®"""
        if self.config.enable_async_write:
            try:
                self.write_queue.put({
                    'file_path': file_path,
                    'data': data,
                    'format_type': format_type,
                    'timestamp': time.time()
                }, timeout=1.0)
            except:
                # é˜Ÿåˆ—æ»¡æ—¶åŒæ­¥å†™å…¥
                self._write_data_sync(file_path, data, format_type)
        else:
            self._write_data_sync(file_path, data, format_type)
    
    def _write_data_sync(self, file_path: str, data: Any, format_type: str):
        """åŒæ­¥å†™å…¥æ•°æ®"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¿…è¦æ—¶åˆ†å‰²æ–‡ä»¶
            actual_path = self._get_actual_file_path(file_path)
            
            # å‡†å¤‡æ•°æ®
            if format_type == 'json':
                content = json.dumps(data, ensure_ascii=False, indent=None)
            else:
                content = str(data)
            
            content += '\n'
            content_bytes = content.encode('utf-8')
            
            # å†™å…¥æ–‡ä»¶
            with self.lock:
                if actual_path.suffix == '.gz':
                    with gzip.open(actual_path, 'at', encoding='utf-8') as f:
                        f.write(content)
                else:
                    with open(actual_path, 'a', encoding='utf-8', buffering=self.config.buffer_size) as f:
                        f.write(content)
                
                # æ›´æ–°æ–‡ä»¶å¤§å°
                if str(actual_path) not in self.file_sizes:
                    self.file_sizes[str(actual_path)] = 0
                self.file_sizes[str(actual_path)] += len(content_bytes)
                
        except Exception as e:
            print(f"å†™å…¥æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    def _get_actual_file_path(self, file_path: str) -> Path:
        """è·å–å®é™…æ–‡ä»¶è·¯å¾„ï¼ˆå¤„ç†æ–‡ä»¶åˆ†å‰²ï¼‰"""
        path = Path(file_path)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if self.config.compression_level > 0 and not path.suffix == '.gz':
            path = path.with_suffix(path.suffix + '.gz')
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶…é™
        current_size = self.file_sizes.get(str(path), 0)
        max_size_bytes = self.config.max_file_size_mb * 1024 * 1024
        
        if current_size > max_size_bytes:
            # éœ€è¦åˆ†å‰²æ–‡ä»¶
            base_name = path.stem
            if path.suffix == '.gz':
                base_name = base_name.rsplit('.', 1)[0]  # å»æ‰.jsonç­‰æ‰©å±•å
            
            counter = self.file_counters.get(str(path), 0) + 1
            self.file_counters[str(path)] = counter
            
            new_name = f"{base_name}_part{counter:03d}.json"
            if self.config.compression_level > 0:
                new_name += '.gz'
            
            path = path.parent / new_name
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        path.parent.mkdir(parents=True, exist_ok=True)
        
        return path
    
    def _write_worker(self):
        """å¼‚æ­¥å†™å…¥å·¥ä½œçº¿ç¨‹"""
        while True:
            try:
                # è·å–å†™å…¥ä»»åŠ¡
                task = self.write_queue.get(timeout=1.0)
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                
                self._write_data_sync(
                    task['file_path'],
                    task['data'],
                    task['format_type']
                )
                
                self.write_queue.task_done()
                
            except Empty:
                continue
            except Exception as e:
                print(f"å¼‚æ­¥å†™å…¥å¼‚å¸¸: {e}")
    
    def flush_all(self):
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº"""
        if self.write_thread and self.write_thread.is_alive():
            # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
            self.write_queue.join()
    
    def close(self):
        """å…³é—­å†™å…¥å™¨"""
        if self.write_thread and self.write_thread.is_alive():
            # å‘é€åœæ­¢ä¿¡å·
            try:
                self.write_queue.put(None, timeout=1.0)
                self.write_thread.join(timeout=5.0)
            except:
                pass
        
        # å…³é—­æ‰€æœ‰æ–‡ä»¶
        with self.lock:
            for writer in self.writers.values():
                try:
                    if hasattr(writer, 'close'):
                        writer.close()
                except:
                    pass
            self.writers.clear()


class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, max_memory_mb: int = 2048):
        self.max_memory_mb = max_memory_mb
        self.process = psutil.Process()
        self.object_pool = ObjectPool(MemoryPool())
        self.stream_writer = StreamWriter(StreamConfig())
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._memory_monitor, daemon=True)
        self.monitor_thread.start()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.gc_count = 0
        self.gc_freed_mb = 0.0
        self.peak_memory_mb = 0.0
    
    def get_memory_usage_mb(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def get_memory_percent(self) -> float:
        """è·å–å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”"""
        return self.get_memory_usage_mb() / self.max_memory_mb
    
    def trigger_gc_if_needed(self) -> float:
        """å¦‚éœ€è¦åˆ™è§¦å‘åƒåœ¾å›æ”¶"""
        memory_percent = self.get_memory_percent()
        
        if memory_percent > 0.8:  # è¶…è¿‡80%æ—¶è§¦å‘
            return self.force_gc()
        return 0.0
    
    def force_gc(self) -> float:
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        before = self.get_memory_usage_mb()
        
        # å¤šè½®åƒåœ¾å›æ”¶
        for i in range(3):
            gc.collect()
        
        after = self.get_memory_usage_mb()
        freed = before - after
        
        self.gc_count += 1
        self.gc_freed_mb += freed
        
        return freed
    
    def optimize_for_large_scale(self, target_records: int) -> Dict[str, Any]:
        """ä¸ºå¤§è§„æ¨¡æ•°æ®ç”Ÿæˆä¼˜åŒ–å†…å­˜"""
        recommendations = {
            'batch_size': min(50000, target_records // 20),
            'gc_frequency': max(1, target_records // 100000),
            'memory_warnings': [],
            'optimizations_applied': []
        }
        
        # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
        system_memory_gb = psutil.virtual_memory().total / 1024**3
        if system_memory_gb < 8:
            recommendations['memory_warnings'].append(
                f"ç³»ç»Ÿå†…å­˜ä»…{system_memory_gb:.1f}GBï¼Œå»ºè®®è‡³å°‘8GBç”¨äºç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆ"
            )
        
        # è°ƒæ•´åƒåœ¾å›æ”¶ç­–ç•¥
        gc.set_threshold(700, 10, 10)  # æ›´æ¿€è¿›çš„GCç­–ç•¥
        recommendations['optimizations_applied'].append("è°ƒæ•´åƒåœ¾å›æ”¶é˜ˆå€¼")
        
        # é¢„åˆ†é…å¯¹è±¡æ± 
        self._preallocate_common_objects()
        recommendations['optimizations_applied'].append("é¢„åˆ†é…å¯¹è±¡æ± ")
        
        return recommendations
    
    def _preallocate_common_objects(self):
        """é¢„åˆ†é…å¸¸ç”¨å¯¹è±¡"""
        # é¢„åˆ†é…å­—å…¸å¯¹è±¡
        for _ in range(1000):
            self.object_pool.return_object('dict', {})
        
        # é¢„åˆ†é…åˆ—è¡¨å¯¹è±¡
        for _ in range(1000):
            self.object_pool.return_object('list', [])
    
    def _memory_monitor(self):
        """å†…å­˜ç›‘æ§çº¿ç¨‹"""
        while self.monitoring:
            try:
                current_memory = self.get_memory_usage_mb()
                self.peak_memory_mb = max(self.peak_memory_mb, current_memory)
                
                # å†…å­˜ä½¿ç”¨è¶…è¿‡90%æ—¶å¼ºåˆ¶GC
                if current_memory / self.max_memory_mb > 0.9:
                    freed = self.force_gc()
                    if freed > 0:
                        print(f"ğŸ§¹ å†…å­˜å‘Šè­¦è§¦å‘GCï¼Œé‡Šæ”¾ {freed:.1f}MB")
                
                time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                print(f"å†…å­˜ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(10)
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'current_memory_mb': self.get_memory_usage_mb(),
            'peak_memory_mb': self.peak_memory_mb,
            'memory_usage_percent': self.get_memory_percent() * 100,
            'gc_count': self.gc_count,
            'total_gc_freed_mb': self.gc_freed_mb,
            'avg_gc_freed_mb': self.gc_freed_mb / max(1, self.gc_count),
            'object_pool_stats': self.object_pool.get_stats(),
            'write_queue_size': self.stream_writer.write_queue.qsize() if self.stream_writer.write_queue else 0
        }
    
    def create_optimized_data_iterator(self, data_generator: callable, 
                                     batch_size: int = 10000) -> Iterator[List[Any]]:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®è¿­ä»£å™¨"""
        batch = []
        
        for item in data_generator():
            batch.append(item)
            
            if len(batch) >= batch_size:
                yield batch
                batch.clear()
                
                # å®šæœŸè§¦å‘GC
                if len(batch) % (batch_size * 5) == 0:
                    self.trigger_gc_if_needed()
        
        # å¤„ç†æœ€åä¸€æ‰¹æ•°æ®
        if batch:
            yield batch
    
    def write_incrementally(self, file_path: str, data: Any, format_type: str = 'json'):
        """å¢é‡å†™å…¥æ•°æ®"""
        self.stream_writer.write_data(file_path, data, format_type)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.monitoring = False
        
        # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        if self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=2.0)
        
        # å…³é—­æµå†™å…¥å™¨
        self.stream_writer.flush_all()
        self.stream_writer.close()
        
        # æœ€ç»ˆåƒåœ¾å›æ”¶
        self.force_gc()