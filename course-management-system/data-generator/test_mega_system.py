#!/usr/bin/env python3
"""
ç®€å•æµ‹è¯•megaç³»ç»Ÿæ˜¯å¦å¯ä»¥æ­£å¸¸å·¥ä½œ
"""

import sys
import yaml
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from mega_scale import MegaDataGenerator
    from mega_scale.mega_generator import MegaGenerationConfig
    print("âœ… æˆåŠŸå¯¼å…¥MegaDataGeneratoræ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥MegaDataGeneratorå¤±è´¥: {e}")
    sys.exit(1)

try:
    # åŠ è½½é…ç½®
    config_file = Path(__file__).parent / "mega_scale_config.yml"
    with open(config_file, 'r', encoding='utf-8') as f:
        yaml_config = yaml.safe_load(f)
    print("âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶")
except Exception as e:
    print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    sys.exit(1)

try:
    # åˆ›å»ºé…ç½®å¯¹è±¡
    generation_cfg = yaml_config.get('generation', {})
    batch_cfg = yaml_config.get('batch_processing', {})
    memory_cfg = yaml_config.get('memory_optimization', {})
    output_cfg = yaml_config.get('output', {})
    
    config = MegaGenerationConfig(
        target_records=10000,  # æµ‹è¯•ç”¨å°æ•°æ®é‡
        batch_size=1000,
        max_memory_mb=512,
        max_workers=2,
        enable_compression=True,
        enable_streaming=True,
        enable_checkpoints=False,  # æµ‹è¯•æ—¶ä¸éœ€è¦æ£€æŸ¥ç‚¹
        output_formats=['json']
    )
    print("âœ… æˆåŠŸåˆ›å»ºç”Ÿæˆé…ç½®")
except Exception as e:
    print(f"âŒ åˆ›å»ºé…ç½®å¤±è´¥: {e}")
    sys.exit(1)

try:
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = MegaDataGenerator(config)
    print("âœ… æˆåŠŸåˆ›å»ºMegaDataGeneratorå®ä¾‹")
except Exception as e:
    print(f"âŒ åˆ›å»ºç”Ÿæˆå™¨å¤±è´¥: {e}")
    sys.exit(1)

print("\nğŸ‰ Megaç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œç™¾ä¸‡çº§æ•°æ®ç”Ÿæˆ")
print(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
print(f"   - ç›®æ ‡è®°å½•æ•°: {config.target_records:,}")
print(f"   - æ‰¹æ¬¡å¤§å°: {config.batch_size:,}")
print(f"   - å†…å­˜é™åˆ¶: {config.max_memory_mb}MB")
print(f"   - å·¥ä½œè¿›ç¨‹: {config.max_workers}")