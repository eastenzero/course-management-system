#!/usr/bin/env python3
"""
æ•°æ®å‘ç°è„šæœ¬ - åˆ†æå·²ç”Ÿæˆçš„æµ‹è¯•æ•°æ®
åŠŸèƒ½ï¼šæ‰«ædata-generatorç›®å½•ï¼Œå‘ç°æ•°æ®æ–‡ä»¶å¹¶æä¾›æ•°æ®é›†é€‰æ‹©å»ºè®®
"""

import os
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

class DataDiscovery:
    """æ•°æ®å‘ç°ç±»"""
    
    def __init__(self, base_dir: str = None):
        """åˆå§‹åŒ–æ•°æ®å‘ç°å™¨"""
        if base_dir is None:
            self.base_dir = Path(__file__).parent / "data-generator"
        else:
            self.base_dir = Path(base_dir)
        
        self.data_directories = [
            self.base_dir / "data_output",
            self.base_dir / "data_output_medium", 
            self.base_dir / "data_output_large",
            self.base_dir / "output"
        ]
    
    def scan_data_files(self) -> Dict[str, Any]:
        """æ‰«ææ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        discovered_files = {}
        
        print("ğŸ” æ‰«ææ•°æ®ç”Ÿæˆå™¨ç›®å½•...")
        print(f"ğŸ“ åŸºç¡€ç›®å½•: {self.base_dir}")
        print("-" * 60)
        
        for data_dir in self.data_directories:
            if not data_dir.exists():
                print(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {data_dir}")
                continue
                
            print(f"ğŸ“‚ æ‰«æç›®å½•: {data_dir}")
            
            # æ‰«æJSONæ–‡ä»¶
            json_files = list(data_dir.rglob("*.json"))
            sql_files = list(data_dir.rglob("*.sql"))
            
            if json_files or sql_files:
                discovered_files[data_dir.name] = {
                    'path': str(data_dir),
                    'json_files': [str(f) for f in json_files],
                    'sql_files': [str(f) for f in sql_files],
                    'file_count': len(json_files) + len(sql_files),
                    'size_mb': self._calculate_dir_size(data_dir)
                }
                
                print(f"   âœ… å‘ç° {len(json_files)} ä¸ªJSONæ–‡ä»¶, {len(sql_files)} ä¸ªSQLæ–‡ä»¶")
                print(f"   ğŸ“Š ç›®å½•å¤§å°: {discovered_files[data_dir.name]['size_mb']:.1f} MB")
            else:
                print(f"   âŒ æœªå‘ç°æ•°æ®æ–‡ä»¶")
        
        return discovered_files
    
    def _calculate_dir_size(self, directory: Path) -> float:
        """è®¡ç®—ç›®å½•å¤§å°(MB)"""
        total_size = 0
        try:
            for file_path in directory.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
            return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        except Exception as e:
            print(f"   âš ï¸  è®¡ç®—ç›®å½•å¤§å°å¤±è´¥: {e}")
            return 0.0
    
    def analyze_data_structure(self, json_file_path: str) -> Dict[str, Any]:
        """åˆ†æJSONæ•°æ®æ–‡ä»¶ç»“æ„"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                # åªè¯»å–å¼€å¤´éƒ¨åˆ†é¿å…å†…å­˜é—®é¢˜
                sample_data = f.read(10000)  # è¯»å–å‰10KB
                f.seek(0)
                
                # å°è¯•è§£æå®Œæ•´JSONï¼ˆå°æ–‡ä»¶ï¼‰æˆ–éƒ¨åˆ†JSONï¼ˆå¤§æ–‡ä»¶ï¼‰
                try:
                    data = json.load(f)
                except (json.JSONDecodeError, MemoryError):
                    # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œåªåˆ†ææ ·æœ¬
                    try:
                        # å°è¯•è§£æéƒ¨åˆ†æ•°æ®
                        data = json.loads(sample_data + "}")
                    except:
                        return {"error": "æ— æ³•è§£æJSONæ–‡ä»¶"}
            
            analysis = {
                'file_path': json_file_path,
                'file_size_mb': Path(json_file_path).stat().st_size / (1024 * 1024),
                'structure': {},
                'record_counts': {},
                'metadata': data.get('metadata', {})
            }
            
            # åˆ†ææ•°æ®ç»“æ„
            for key, value in data.items():
                if isinstance(value, list):
                    analysis['record_counts'][key] = len(value)
                    if value:  # å¦‚æœåˆ—è¡¨éç©ºï¼Œåˆ†æç¬¬ä¸€ä¸ªå…ƒç´ çš„ç»“æ„
                        analysis['structure'][key] = list(value[0].keys()) if isinstance(value[0], dict) else type(value[0]).__name__
                elif isinstance(value, dict):
                    analysis['structure'][key] = list(value.keys())
                else:
                    analysis['structure'][key] = type(value).__name__
            
            return analysis
            
        except Exception as e:
            return {"error": f"åˆ†æå¤±è´¥: {str(e)}"}
    
    def recommend_dataset(self, discovered_files: Dict[str, Any]) -> Optional[str]:
        """æ¨èæœ€ä½³æ•°æ®é›†"""
        print("\nğŸ¯ æ•°æ®é›†åˆ†æä¸æ¨è...")
        print("-" * 60)
        
        recommendations = []
        
        for dir_name, file_info in discovered_files.items():
            if not file_info['json_files']:
                continue
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªJSONæ–‡ä»¶è¿›è¡Œåˆ†æ
            json_file = file_info['json_files'][0]
            analysis = self.analyze_data_structure(json_file)
            
            if 'error' in analysis:
                print(f"âŒ {dir_name}: {analysis['error']}")
                continue
            
            total_records = sum(analysis['record_counts'].values())
            score = self._calculate_recommendation_score(file_info, analysis)
            
            recommendations.append({
                'dir_name': dir_name,
                'file_path': json_file,
                'total_records': total_records,
                'file_size_mb': analysis['file_size_mb'],
                'score': score,
                'metadata': analysis.get('metadata', {}),
                'record_counts': analysis['record_counts']
            })
            
            print(f"ğŸ“Š {dir_name}:")
            print(f"   ğŸ“ æ–‡ä»¶: {Path(json_file).name}")
            print(f"   ğŸ“ å¤§å°: {analysis['file_size_mb']:.1f} MB")
            print(f"   ğŸ“ˆ è®°å½•æ•°: {total_records:,}")
            print(f"   ğŸ† æ¨èåˆ†æ•°: {score:.1f}/10")
            
            # æ˜¾ç¤ºå„ç±»å‹è®°å½•æ•°é‡
            if analysis['record_counts']:
                print("   ğŸ“‹ æ•°æ®è¯¦æƒ…:")
                for data_type, count in analysis['record_counts'].items():
                    if data_type != 'metadata':
                        print(f"      - {data_type}: {count:,} æ¡")
        
        if not recommendations:
            print("âŒ æœªå‘ç°å¯ç”¨çš„æ•°æ®é›†")
            return None
        
        # æŒ‰åˆ†æ•°æ’åº
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        best_recommendation = recommendations[0]
        
        print(f"\nğŸ† æ¨èæ•°æ®é›†: {best_recommendation['dir_name']}")
        print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {best_recommendation['file_path']}")
        print(f"   ğŸ¯ æ¨èç†ç”±: æ•°æ®é‡é€‚ä¸­ï¼Œè´¨é‡è¾ƒå¥½ï¼Œé€‚åˆæ¼”ç¤ºå’Œæµ‹è¯•")
        
        return best_recommendation['file_path']
    
    def _calculate_recommendation_score(self, file_info: Dict, analysis: Dict) -> float:
        """è®¡ç®—æ¨èåˆ†æ•° (0-10åˆ†)"""
        score = 5.0  # åŸºç¡€åˆ†æ•°
        
        # æ–‡ä»¶å¤§å°è¯„åˆ† (é€‚ä¸­çš„æ–‡ä»¶å¤§å°å¾—åˆ†æ›´é«˜)
        size_mb = analysis['file_size_mb']
        if 10 <= size_mb <= 100:  # 10-100MB æœ€ä½³
            score += 2.0
        elif 1 <= size_mb <= 200:  # 1-200MB è‰¯å¥½
            score += 1.0
        elif size_mb > 500:  # è¶…è¿‡500MB æ‰£åˆ†
            score -= 1.0
        
        # è®°å½•æ•°é‡è¯„åˆ†
        total_records = sum(analysis['record_counts'].values())
        if 1000 <= total_records <= 100000:  # 1K-100Kè®°å½•æœ€ä½³
            score += 2.0
        elif 100 <= total_records <= 200000:  # 100-200Kè®°å½•è‰¯å¥½
            score += 1.0
        
        # æ•°æ®å®Œæ•´æ€§è¯„åˆ†
        required_tables = ['departments', 'students', 'teachers', 'courses', 'enrollments']
        existing_tables = set(analysis['record_counts'].keys())
        completeness = len(existing_tables.intersection(required_tables)) / len(required_tables)
        score += completeness * 2.0
        
        # å…ƒæ•°æ®è¯„åˆ†
        if analysis.get('metadata') and 'validation_passed' in analysis['metadata']:
            if analysis['metadata'].get('validation_passed', False):
                score += 1.0
        
        return min(score, 10.0)  # æœ€é«˜10åˆ†
    
    def generate_report(self, discovered_files: Dict[str, Any], recommended_file: str = None) -> None:
        """ç”Ÿæˆå‘ç°æŠ¥å‘Š"""
        print(f"\nğŸ“‹ æ•°æ®å‘ç°æŠ¥å‘Š")
        print("=" * 60)
        print(f"ğŸ• æ‰«ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“‚ æ‰«æç›®å½•æ•°: {len(self.data_directories)}")
        print(f"ğŸ“ å‘ç°æ•°æ®é›†: {len(discovered_files)}")
        
        if recommended_file:
            print(f"ğŸ¯ æ¨èæ•°æ®é›†: {Path(recommended_file).parent.name}")
            print(f"ğŸ“„ æ¨èæ–‡ä»¶: {Path(recommended_file).name}")
        
        print("\nğŸ“Š æ•°æ®é›†æ¦‚è§ˆ:")
        total_size = 0
        total_files = 0
        
        for dir_name, info in discovered_files.items():
            print(f"  ğŸ“ {dir_name}:")
            print(f"     ğŸ“„ æ–‡ä»¶æ•°: {info['file_count']}")
            print(f"     ğŸ“ å¤§å°: {info['size_mb']:.1f} MB")
            total_size += info['size_mb']
            total_files += info['file_count']
        
        print(f"\nğŸ“ˆ æ€»è®¡:")
        print(f"  ğŸ“„ æ–‡ä»¶æ€»æ•°: {total_files}")
        print(f"  ğŸ“ æ€»å¤§å°: {total_size:.1f} MB")
        
        if recommended_file:
            print(f"\nâœ… å»ºè®®ä½¿ç”¨æ•°æ®é›†: {recommended_file}")
        else:
            print(f"\nâŒ æœªæ‰¾åˆ°åˆé€‚çš„æ•°æ®é›†")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¯¾ç¨‹ç®¡ç†ç³»ç»Ÿ - æ•°æ®å‘ç°å·¥å…·")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®å‘ç°å™¨
    discovery = DataDiscovery()
    
    # æ‰«ææ•°æ®æ–‡ä»¶
    discovered_files = discovery.scan_data_files()
    
    if not discovered_files:
        print("\nâŒ æœªå‘ç°ä»»ä½•æ•°æ®æ–‡ä»¶ï¼")
        print("è¯·ç¡®ä¿æ•°æ®ç”Ÿæˆå™¨å·²è¿è¡Œå¹¶ç”Ÿæˆäº†æµ‹è¯•æ•°æ®ã€‚")
        return False
    
    # æ¨èæ•°æ®é›†
    recommended_file = discovery.recommend_dataset(discovered_files)
    
    # ç”ŸæˆæŠ¥å‘Š
    discovery.generate_report(discovered_files, recommended_file)
    
    return recommended_file


if __name__ == "__main__":
    recommended_file = main()
    if recommended_file:
        print(f"\nğŸ‰ æ•°æ®å‘ç°å®Œæˆï¼æ¨èæ•°æ®æ–‡ä»¶ï¼š")
        print(f"ğŸ“„ {recommended_file}")
        
        # å°†æ¨èæ–‡ä»¶è·¯å¾„ä¿å­˜åˆ°ç¯å¢ƒå˜é‡æ–‡ä»¶
        env_file = Path(__file__).parent / ".recommended_data_file"
        with open(env_file, 'w') as f:
            f.write(recommended_file)
        print(f"ğŸ’¾ æ¨èè·¯å¾„å·²ä¿å­˜åˆ°: {env_file}")
    else:
        sys.exit(1)