"""
ç®—æ³•æ€§èƒ½å¯¹æ¯”å·¥å…·
ç”¨äºæ¯”è¾ƒä¸åŒæ’è¯¾ç®—æ³•çš„æ€§èƒ½
"""

import time
import json
from typing import Dict, List, Any
from datetime import datetime

from .algorithms import create_auto_schedule
from .genetic_algorithm import create_genetic_schedule
from .hybrid_algorithm import create_hybrid_schedule
from apps.courses.models import Course

class AlgorithmPerformanceComparator:
    """ç®—æ³•æ€§èƒ½å¯¹æ¯”å™¨"""
    
    def __init__(self, semester: str, academic_year: str):
        self.semester = semester
        self.academic_year = academic_year
        self.results = []
        
    def compare_algorithms(self, course_ids: List[int] = None, timeout_seconds: int = 300) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸åŒç®—æ³•çš„æ€§èƒ½
        
        Args:
            course_ids: è¦æ’è¯¾çš„è¯¾ç¨‹IDåˆ—è¡¨
            timeout_seconds: ç®—æ³•æ‰§è¡Œè¶…æ—¶æ—¶é—´
            
        Returns:
            æ€§èƒ½å¯¹æ¯”ç»“æœ
        """
        algorithms = [
            ('è´ªå¿ƒç®—æ³•', 'greedy', create_auto_schedule),
            ('é—ä¼ ç®—æ³•', 'genetic', create_genetic_schedule),
            ('æ··åˆç®—æ³•', 'hybrid', create_hybrid_schedule)
        ]
        
        comparison_results = {
            'algorithms': [],
            'comparison': {},
            'timestamp': datetime.now().isoformat(),
            'semester': self.semester,
            'academic_year': self.academic_year
        }
        
        # è·å–è¯¾ç¨‹æ€»æ•°ç”¨äºç»Ÿè®¡
        courses_query = Course.objects.filter(
            semester=self.semester,
            academic_year=self.academic_year,
            is_active=True,
            is_published=True
        )
        if course_ids:
            courses_query = courses_query.filter(id__in=course_ids)
        
        total_courses = courses_query.count()
        
        # åˆ†åˆ«è¿è¡Œæ¯ä¸ªç®—æ³•
        for algorithm_name, algorithm_type, algorithm_func in algorithms:
            print(f"ğŸ”„ æ­£åœ¨è¿è¡Œ{algorithm_name}...")
            
            try:
                start_time = time.time()
                
                # è¿è¡Œç®—æ³•
                if algorithm_type == 'greedy':
                    result = algorithm_func(self.semester, self.academic_year, course_ids, algorithm_type, timeout_seconds)
                else:
                    result = algorithm_func(self.semester, self.academic_year, course_ids)
                
                execution_time = time.time() - start_time
                
                # æå–å…³é”®æŒ‡æ ‡
                success_rate = result.get('success_rate', 0)
                successful_assignments = result.get('successful_assignments', 0)
                total_constraints = result.get('total_constraints', 0)
                failed_assignments = len(result.get('failed_assignments', []))
                
                # è®¡ç®—èµ„æºåˆ©ç”¨ç‡æŒ‡æ ‡
                resource_utilization = result.get('resource_utilization', {})
                classroom_usage = resource_utilization.get('classroom_usage', {})
                teacher_workload = resource_utilization.get('teacher_workload', {})
                
                # è®¡ç®—æ•™å®¤åˆ©ç”¨ç‡å¹³è¡¡åº¦
                classroom_balance = self._calculate_balance_score(list(classroom_usage.values())) if classroom_usage else 0
                
                # è®¡ç®—æ•™å¸ˆå·¥ä½œé‡å¹³è¡¡åº¦
                teacher_balance = self._calculate_balance_score(list(teacher_workload.values())) if teacher_workload else 0
                
                algorithm_result = {
                    'name': algorithm_name,
                    'type': algorithm_type,
                    'execution_time': execution_time,
                    'success_rate': success_rate,
                    'successful_assignments': successful_assignments,
                    'total_constraints': total_constraints,
                    'failed_assignments': failed_assignments,
                    'classroom_balance': classroom_balance,
                    'teacher_balance': teacher_balance,
                    'total_courses': total_courses,
                    'status': 'completed'
                }
                
                comparison_results['algorithms'].append(algorithm_result)
                print(f"  âœ… {algorithm_name}å®Œæˆ: æˆåŠŸç‡{success_rate:.1f}%, è€—æ—¶{execution_time:.2f}ç§’")
                
            except Exception as e:
                print(f"  âŒ {algorithm_name}å¤±è´¥: {str(e)}")
                algorithm_result = {
                    'name': algorithm_name,
                    'type': algorithm_type,
                    'execution_time': 0,
                    'success_rate': 0,
                    'successful_assignments': 0,
                    'total_constraints': 0,
                    'failed_assignments': 0,
                    'classroom_balance': 0,
                    'teacher_balance': 0,
                    'total_courses': total_courses,
                    'status': 'failed',
                    'error': str(e)
                }
                comparison_results['algorithms'].append(algorithm_result)
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        comparison_results['comparison'] = self._generate_comparison_analysis(comparison_results['algorithms'])
        
        return comparison_results
    
    def _calculate_balance_score(self, values: List[float]) -> float:
        """
        è®¡ç®—å¹³è¡¡åº¦åˆ†æ•°
        
        Args:
            values: æ•°å€¼åˆ—è¡¨
            
        Returns:
            å¹³è¡¡åº¦åˆ†æ•° (0-1, 1è¡¨ç¤ºå®Œå…¨å¹³è¡¡)
        """
        if not values:
            return 0
            
        if len(values) == 1:
            return 1
            
        # è®¡ç®—æ–¹å·®
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        
        # è½¬æ¢ä¸ºå¹³è¡¡åº¦åˆ†æ•° (æ–¹å·®è¶Šå°ï¼Œå¹³è¡¡åº¦è¶Šé«˜)
        if mean == 0:
            return 1 if variance == 0 else 0
            
        # ä½¿ç”¨å˜å¼‚ç³»æ•°çš„å€’æ•°æ¥è®¡ç®—å¹³è¡¡åº¦
        coefficient_of_variation = (variance ** 0.5) / mean
        balance_score = max(0, 1 - min(1, coefficient_of_variation))
        
        return balance_score
    
    def _generate_comparison_analysis(self, algorithm_results: List[Dict]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¯¹æ¯”åˆ†æ
        
        Args:
            algorithm_results: ç®—æ³•ç»“æœåˆ—è¡¨
            
        Returns:
            å¯¹æ¯”åˆ†æç»“æœ
        """
        if not algorithm_results:
            return {}
        
        # æ‰¾åˆ°æœ€ä½³ç®—æ³•
        completed_algorithms = [r for r in algorithm_results if r['status'] == 'completed']
        
        if not completed_algorithms:
            return {'best_algorithm': None, 'analysis': 'æ‰€æœ‰ç®—æ³•éƒ½æ‰§è¡Œå¤±è´¥'}
        
        # æŒ‰æˆåŠŸç‡æ’åº
        best_by_success = max(completed_algorithms, key=lambda x: x['success_rate'])
        
        # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
        best_by_time = min(completed_algorithms, key=lambda x: x['execution_time'])
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº (æˆåŠŸç‡70% + æ‰§è¡Œæ—¶é—´30%)
        def composite_score(result):
            time_score = 1 - min(1, result['execution_time'] / 300)  # å‡è®¾300ç§’ä¸ºæœ€å¤§å¯æ¥å—æ—¶é—´
            return result['success_rate'] * 0.7 + time_score * 30  # æ—¶é—´æƒé‡è½¬æ¢ä¸º0-30åˆ†
        
        best_overall = max(completed_algorithms, key=composite_score)
        
        return {
            'best_by_success_rate': {
                'algorithm': best_by_success['name'],
                'success_rate': best_by_success['success_rate'],
                'reason': f'{best_by_success["name"]}è¾¾åˆ°äº†æœ€é«˜çš„æˆåŠŸç‡'
            },
            'best_by_execution_time': {
                'algorithm': best_by_time['name'],
                'execution_time': best_by_time['execution_time'],
                'reason': f'{best_by_time["name"]}æ‰§è¡Œé€Ÿåº¦æœ€å¿«'
            },
            'best_overall': {
                'algorithm': best_overall['name'],
                'success_rate': best_overall['success_rate'],
                'execution_time': best_overall['execution_time'],
                'reason': f'{best_overall["name"]}åœ¨æˆåŠŸç‡å’Œæ‰§è¡Œæ—¶é—´ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡'
            }
        }
    
    def generate_detailed_report(self, comparison_results: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
        
        Args:
            comparison_results: å¯¹æ¯”ç»“æœ
            
        Returns:
            è¯¦ç»†æŠ¥å‘Šæ–‡æœ¬
        """
        report = []
        report.append("=" * 80)
        report.append("æ™ºèƒ½æ’è¯¾ç®—æ³•æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"ç”Ÿæˆæ—¶é—´: {comparison_results['timestamp']}")
        report.append(f"å­¦æœŸ: {comparison_results['semester']}")
        report.append(f"å­¦å¹´: {comparison_results['academic_year']}")
        report.append("")
        
        # ç®—æ³•è¯¦ç»†ç»“æœ
        report.append("ç®—æ³•è¯¦ç»†ç»“æœ:")
        report.append("-" * 80)
        report.append(f"{'ç®—æ³•åç§°':<12} {'çŠ¶æ€':<8} {'æˆåŠŸç‡':<8} {'æˆåŠŸæ•°':<8} {'å¤±è´¥æ•°':<8} {'è€—æ—¶(ç§’)':<10} {'æ•™å®¤å¹³è¡¡':<8} {'æ•™å¸ˆå¹³è¡¡':<8}")
        report.append("-" * 80)
        
        for result in comparison_results['algorithms']:
            if result['status'] == 'completed':
                report.append(f"{result['name']:<12} {result['status']:<8} {result['success_rate']:<8.1f} "
                            f"{result['successful_assignments']:<8} {result['failed_assignments']:<8} "
                            f"{result['execution_time']:<10.2f} {result['classroom_balance']:<8.2f} "
                            f"{result['teacher_balance']:<8.2f}")
            else:
                report.append(f"{result['name']:<12} {result['status']:<8} {'å¤±è´¥':<8} {'-':<8} {'-':<8} {'-':<10} {'-':<8} {'-':<8}")
        
        report.append("")
        
        # å¯¹æ¯”åˆ†æ
        comparison = comparison_results['comparison']
        if comparison:
            report.append("å¯¹æ¯”åˆ†æ:")
            report.append("-" * 40)
            if 'best_by_success_rate' in comparison:
                best_success = comparison['best_by_success_rate']
                report.append(f"æœ€é«˜æˆåŠŸç‡: {best_success['algorithm']} ({best_success['success_rate']:.1f}%)")
                report.append(f"  åŸå› : {best_success['reason']}")
            
            if 'best_by_execution_time' in comparison:
                best_time = comparison['best_by_execution_time']
                report.append(f"æœ€å¿«æ‰§è¡Œ: {best_time['algorithm']} ({best_time['execution_time']:.2f}ç§’)")
                report.append(f"  åŸå› : {best_time['reason']}")
            
            if 'best_overall' in comparison:
                best_overall = comparison['best_overall']
                report.append(f"ç»¼åˆæœ€ä½³: {best_overall['algorithm']}")
                report.append(f"  æˆåŠŸç‡: {best_overall['success_rate']:.1f}%")
                report.append(f"  æ‰§è¡Œæ—¶é—´: {best_overall['execution_time']:.2f}ç§’")
                report.append(f"  åŸå› : {best_overall['reason']}")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)

def run_performance_comparison(semester: str, academic_year: str, course_ids: List[int] = None, 
                             timeout_seconds: int = 300) -> Dict[str, Any]:
    """
    è¿è¡Œç®—æ³•æ€§èƒ½å¯¹æ¯”
    
    Args:
        semester: å­¦æœŸ
        academic_year: å­¦å¹´
        course_ids: è¯¾ç¨‹IDåˆ—è¡¨
        timeout_seconds: è¶…æ—¶æ—¶é—´
        
    Returns:
        å¯¹æ¯”ç»“æœ
    """
    comparator = AlgorithmPerformanceComparator(semester, academic_year)
    results = comparator.compare_algorithms(course_ids, timeout_seconds)
    
    # ç”Ÿæˆå¹¶æ‰“å°è¯¦ç»†æŠ¥å‘Š
    report = comparator.generate_detailed_report(results)
    print(report)
    
    return results